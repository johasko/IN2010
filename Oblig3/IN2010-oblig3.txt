Oppgave 1 del 3:

I hvilken grad stemmer kjøretiden overens med kjøretidsanalysene (store O) for de ulike algoritmene?
Quick: O(n^2) (ofte O(nlog(n))
Bubble: O(n^2)
Insertion: O(n^2)
Heap: O(nlog(n))

Vår oppfatning er at resultatene stort sett stemmer overens med kjøretidsanalysen for algoritmene.
Vi opplever i flere tilfeller at quick sort opererer i O(nlog(n)) enn worst case O(n^2) (Se figur .. og ..). Dette er sannsynligvis fordi quick sort vanligvis kjører med denne kompleksiteten, og sjelden når forutsetningene for worst case. Ut over dette stemmer resultatene relativt godt med forventningene. 

Hvilke sorteringsalgoritmer utmerker seg positivt når n er veldig liten? Og når n er veldig stor?
Med veldig liten n utmerker insertion og bubble sort seg positivt (se random_/nearly_sorted_10), som de mest effektive algoritmene. Når det gjelder stor n, ser vi at quick og heap sort skiller seg positivt ut fremfor de andre (se random_/nearly_sorted_n > 100). Vi ser riktignok på analysen av nearly_sorted at insertion sort også utmerker seg positivt (nearly_sorted_10000). Dette skyldes sannsynligvis at en nesten sortert liste nærmer seg forutsetningene for beste-scenario kjøretid for algoritmen.

Hvilke sorteringsalgoritmer utmerker seg positivt for de ulike inputfilene?
For samtlige nearly_sorted-filer ser vi at insertion sort gjør det svært bra. Dette skyldes nok at en nesten sortert mengde tall krever liten grad av sortering før innsetting. Dermed vil insertion sort håndtere datamengden effektivt. 

På inputfilene med verdi > 10, ser vi at heap og quick sort er svært effektive og overlegne mtp. tid. Motsatt gjelder, som nevnt over, for insertion og bubble sort for n = 10.

Har du noen overraskende funn å rapportere?
Insertion sort fungerer bra uansett størrelse på n, så lenge listen er helt eller nesten sortert. Dette fordi det er få, og/eller små bytter som må finne plass, og dermed forsvinner lite tid til dette. 

###OBS: Grunnet bruddverdier i prekoden ble enkelte algoritmer avbrutt pga. tid ved større n. Dette kan man bl.a. se på random_10000, hvor noen av kurvene stanser før andre, i tillegg til at x-aksen ikke strekker seg helt til verdien av n. Vi ser likevel, til tross for dette, tendensene i utviklingen, og føler at selv de uferdige resultatene gir oss et god inntrykk av algoritmens kjøretid.###

Oppgave 2:
1)
Din første oppgave er å beregne en øvre grense for hvor mye kabel selskapet trenger. Hva er kostnaden av den dyreste måten å koble sammen alle signaltårn på? Oppgi svaret ditt som et tall.
Dyreste måte: 61. 

T1->T2->T4->T6->T7->T5->T3

2)
Hva er den billigste måten å koble sammen alle signaltårn på? Oppgi dette som et tall. Videre, oppgi hvilken algoritme du brukte, samt hvordan du formaliserte problemet. Til slutt, oppgi hvilke andre algoritmer som er blitt dekket i pensum, som man kunne ha brukt i stedet.
Billigste måte: 34. 
			  T3
			/
T1 -> T4 -> T2 -> T5 -> 
			\
			  T6 -> T7

Vi brukte Prims for å finne billigste vei her. Vi så på grafen som et vektet spenntre, og så det derfor som formålsmessig å bruke Prims algoritme for å løse problemet. Vi kunne alternativt brukt Kruskal eller Borůvka, men siden vi valgte å tegne opp og løse problemet "manuelt", syntes vi det var enklest å anvende fremgangsmåten i Prims.

Oppgave 3;

Algoritme 1:

Algoritme 2:

Algoritme 3:
